{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c3313f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MOEIZ.XTREMELITE-PC\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# For Data Processing\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "# For ML Models\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.losses import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.metrics import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.applications import *\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "model_file = 'cnn_model2.sav'\n",
    "\n",
    "training_folder = 'Training'\n",
    "testing_folder = 'Testing'\n",
    "\n",
    "training_images_paths = []\n",
    "training_images_target = []\n",
    "\n",
    "\n",
    "# Our training model path is  Training_\n",
    "for label in os.listdir(training_folder):             \n",
    "    for image in os.listdir(training_folder+\"/\" +label):\n",
    "        training_images_paths.append(training_folder + '/'+label+ '/'+image)\n",
    "        if(label=='YES'):\n",
    "            training_images_target.append(1)\n",
    "        elif(label=='NO'):\n",
    "            training_images_target.append(0)\n",
    "\n",
    "\n",
    "            \n",
    "testing_images_paths = []\n",
    "testing_images_target = []\n",
    "\n",
    "for label in os.listdir(testing_folder):\n",
    "    for image in os.listdir(testing_folder+ '/' + label):\n",
    "        testing_images_paths.append(testing_folder +'/'+label + '/'+image)\n",
    "        if(label=='YES'):\n",
    "            testing_images_target.append(1)\n",
    "        elif(label=='NO'):\n",
    "            testing_images_target.append(0)\n",
    "\n",
    "            \n",
    "testing_images_paths, testing_images_target = shuffle(testing_images_paths, testing_images_target)\n",
    "\n",
    "\n",
    "training_images_paths, training_images_target = shuffle(training_images_paths, training_images_target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26ad978d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def extracting_images(paths):\n",
    "\n",
    "    images = []\n",
    "    for path in paths:\n",
    "        image = load_img(path, target_size=(200, 200))\n",
    "        image = Image.fromarray(np.uint8(image))\n",
    "        image = np.array(image)/255.0\n",
    "        images.append(image)\n",
    "    return np.array(images)\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c40b6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_25 (Conv2D)          (None, 198, 198, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_25 (MaxPoolin  (None, 99, 99, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_26 (Conv2D)          (None, 97, 97, 36)        5220      \n",
      "                                                                 \n",
      " max_pooling2d_26 (MaxPoolin  (None, 48, 48, 36)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 82944)             0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 128)               10616960  \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,622,757\n",
      "Trainable params: 10,622,757\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(200, 200, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.add(Conv2D(36, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ae1a805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "178/178 [==============================] - 132s 741ms/step - loss: 0.0221 - accuracy: 0.9917\n",
      "Epoch 2/5\n",
      "178/178 [==============================] - 132s 744ms/step - loss: 0.0264 - accuracy: 0.9884\n",
      "Epoch 3/5\n",
      "178/178 [==============================] - 134s 752ms/step - loss: 0.0160 - accuracy: 0.9935\n",
      "Epoch 4/5\n",
      "178/178 [==============================] - 134s 755ms/step - loss: 0.0175 - accuracy: 0.9931\n",
      "Epoch 5/5\n",
      "178/178 [==============================] - 133s 747ms/step - loss: 0.0104 - accuracy: 0.9947\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\\conv2d\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\conv2d_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dropout\n",
      "......vars\n",
      "...layers\\flatten\n",
      "......vars\n",
      "...layers\\max_pooling2d\n",
      "......vars\n",
      "...layers\\max_pooling2d_1\n",
      "......vars\n",
      "...metrics\\mean\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...metrics\\mean_metric_wrapper\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........10\n",
      ".........11\n",
      ".........12\n",
      ".........13\n",
      ".........14\n",
      ".........15\n",
      ".........16\n",
      ".........2\n",
      ".........3\n",
      ".........4\n",
      ".........5\n",
      ".........6\n",
      ".........7\n",
      ".........8\n",
      ".........9\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2022-12-25 21:26:11         3246\n",
      "metadata.json                                  2022-12-25 21:26:11           64\n",
      "variables.h5                                   2022-12-25 21:26:12    127507000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def batch_creation(paths, labels, batch_size, epochs):\n",
    "    for i in range(0, epochs):\n",
    "\n",
    "        for pathindex in range(0, len(paths), batch_size):\n",
    "            batch_paths = paths[pathindex:pathindex+batch_size] #This would create batches of given size\n",
    "            batch_images = extracting_images(batch_paths)\n",
    "            batch_labels = labels[pathindex:pathindex+batch_size]\n",
    "            batch_labels=np.array(batch_labels)\n",
    "            yield batch_images, batch_labels\n",
    "\n",
    "model.fit(batch_creation(training_images_paths, training_images_target, batch_size=32,epochs=5),steps_per_epoch=178, epochs=5)\n",
    "pickle.dump(model, open(model_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f14ddfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2022-12-25 21:27:00         3246\n",
      "metadata.json                                  2022-12-25 21:27:00           64\n",
      "variables.h5                                   2022-12-25 21:27:00    127507000\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\\conv2d\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\conv2d_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dropout\n",
      "......vars\n",
      "...layers\\flatten\n",
      "......vars\n",
      "...layers\\max_pooling2d\n",
      "......vars\n",
      "...layers\\max_pooling2d_1\n",
      "......vars\n",
      "...metrics\\mean\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...metrics\\mean_metric_wrapper\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........10\n",
      ".........11\n",
      ".........12\n",
      ".........13\n",
      ".........14\n",
      ".........15\n",
      ".........16\n",
      ".........2\n",
      ".........3\n",
      ".........4\n",
      ".........5\n",
      ".........6\n",
      ".........7\n",
      ".........8\n",
      ".........9\n",
      "...vars\n",
      "41/41 [==============================] - 13s 190ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       414\n",
      "           1       0.99      1.00      1.00       897\n",
      "\n",
      "    accuracy                           0.99      1311\n",
      "   macro avg       1.00      0.99      0.99      1311\n",
      "weighted avg       0.99      0.99      0.99      1311\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_model = pickle.load(open(model_file, 'rb'))\n",
    "\n",
    "predictions=loaded_model.predict(extracting_images(testing_images_paths))\n",
    "model_test_predictions=[]\n",
    "\n",
    "for i in range(0,len(predictions)):\n",
    "    if predictions[i]>=0.5:\n",
    "        model_test_predictions.append(1)\n",
    "    else:\n",
    "        model_test_predictions.append(0)\n",
    "\n",
    "\n",
    "print(classification_report(model_test_predictions,testing_images_target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcebd458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.17647059 0.17647059 0.17647059]\n",
      "  [0.17254902 0.17254902 0.17254902]\n",
      "  [0.17254902 0.17254902 0.17254902]\n",
      "  ...\n",
      "  [0.19215686 0.19215686 0.19215686]\n",
      "  [0.18823529 0.18823529 0.18823529]\n",
      "  [0.19215686 0.19215686 0.19215686]]\n",
      "\n",
      " [[0.18431373 0.18431373 0.18431373]\n",
      "  [0.18431373 0.18431373 0.18431373]\n",
      "  [0.18431373 0.18431373 0.18431373]\n",
      "  ...\n",
      "  [0.18823529 0.18823529 0.18823529]\n",
      "  [0.19215686 0.19215686 0.19215686]\n",
      "  [0.20392157 0.20392157 0.20392157]]\n",
      "\n",
      " [[0.18823529 0.18823529 0.18823529]\n",
      "  [0.18431373 0.18431373 0.18431373]\n",
      "  [0.18039216 0.18039216 0.18039216]\n",
      "  ...\n",
      "  [0.2        0.2        0.2       ]\n",
      "  [0.19215686 0.19215686 0.19215686]\n",
      "  [0.19607843 0.19607843 0.19607843]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.51372549 0.51372549 0.51372549]\n",
      "  [0.76862745 0.76862745 0.76862745]\n",
      "  [0.78431373 0.78431373 0.78431373]\n",
      "  ...\n",
      "  [0.19607843 0.19607843 0.19607843]\n",
      "  [0.20392157 0.20392157 0.20392157]\n",
      "  [0.21568627 0.21568627 0.21568627]]\n",
      "\n",
      " [[0.61176471 0.61176471 0.61176471]\n",
      "  [0.98431373 0.98431373 0.98431373]\n",
      "  [0.58431373 0.58431373 0.58431373]\n",
      "  ...\n",
      "  [0.19607843 0.19607843 0.19607843]\n",
      "  [0.19607843 0.19607843 0.19607843]\n",
      "  [0.20784314 0.20784314 0.20784314]]\n",
      "\n",
      " [[0.61568627 0.61568627 0.61568627]\n",
      "  [0.98823529 0.98823529 0.98823529]\n",
      "  [0.34901961 0.34901961 0.34901961]\n",
      "  ...\n",
      "  [0.19215686 0.19215686 0.19215686]\n",
      "  [0.19607843 0.19607843 0.19607843]\n",
      "  [0.20392157 0.20392157 0.20392157]]]\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "Tumor was detected\n"
     ]
    }
   ],
   "source": [
    "path=\"F:/Classification Project/training_set/hemmorhage_data/098.png\"\n",
    "\n",
    "\n",
    "predictionimages = [] #The model takes a numpy array\n",
    "image = load_img(path, target_size=(200, 200))\n",
    "image = Image.fromarray(np.uint8(image))\n",
    "\n",
    "image = np.array(image)/255.0\n",
    "\n",
    "predictionimages.append(image)\n",
    "predictionimages=np.array(predictionimages)\n",
    "\n",
    "\n",
    "predictions = loaded_model.predict(predictionimages)\n",
    "if(predictions[0][0]<0.5):\n",
    "    print(\"Tumor was not detected\")\n",
    "elif(predictions[0][0]>=0.5):\n",
    "    print(\"Tumor was detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2668c9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc576079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\\conv2d\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\conv2d_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dropout\n",
      "......vars\n",
      "...layers\\flatten\n",
      "......vars\n",
      "...layers\\max_pooling2d\n",
      "......vars\n",
      "...layers\\max_pooling2d_1\n",
      "......vars\n",
      "...metrics\\mean\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...metrics\\mean_metric_wrapper\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........10\n",
      ".........11\n",
      ".........12\n",
      ".........13\n",
      ".........14\n",
      ".........15\n",
      ".........16\n",
      ".........2\n",
      ".........3\n",
      ".........4\n",
      ".........5\n",
      ".........6\n",
      ".........7\n",
      ".........8\n",
      ".........9\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2022-12-25 21:27:00         3246\n",
      "metadata.json                                  2022-12-25 21:27:00           64\n",
      "variables.h5                                   2022-12-25 21:27:01    127507000\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cb8aee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "bfe28f43e63747169456204833a34807467689a77414e12e61f13c455de3f81d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
